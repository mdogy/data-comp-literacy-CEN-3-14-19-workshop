{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:27.105799Z",
     "start_time": "2018-11-25T06:45:25.136803Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests # Download stuff over the web\n",
    "import pandas as pd # Makes some nice tables\n",
    "import nltk # Clean up words, word counts\n",
    "nltk.download('stopwords') # Make sure stopwords downloaded\n",
    "from nltk.corpus import stopwords # Make stopwords available\n",
    "from wordcloud import WordCloud # Make word cloud images\n",
    "from matplotlib import pyplot as plt # Make figures\n",
    "# This command display figures inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:27.469379Z",
     "start_time": "2018-11-25T06:45:27.109661Z"
    }
   },
   "outputs": [],
   "source": [
    "#treasure island, jekyll and hyde, moby dick\n",
    "ti_url = 'https://www.gutenberg.org/files/120/120-0.txt'\n",
    "jh_url = 'https://www.gutenberg.org/files/43/43-0.txt'\n",
    "md_url = 'https://www.gutenberg.org/files/2701/2701-0.txt'\n",
    "req = requests.get(ti_url)\n",
    "print(req.text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:27.501819Z",
     "start_time": "2018-11-25T06:45:27.474045Z"
    }
   },
   "outputs": [],
   "source": [
    "ti_text = (req.text.split(\n",
    "    'START OF THIS PROJECT GUTENBERG EBOOK'))[-1]\n",
    "ti_text = ti_text.strip()\n",
    "ti_text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:27.538736Z",
     "start_time": "2018-11-25T06:45:27.505567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Project Gutenberg Header\n",
    "ti_text = (req.text.split(\n",
    "    'START OF THIS PROJECT GUTENBERG EBOOK'))[-1]\n",
    "ti_text = ti_text.strip()\n",
    "ti_text = '\\n'.join(ti_text.split('\\r\\n')[6:])\n",
    "ti_text = ti_text.strip()\n",
    "print(ti_text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:27.581094Z",
     "start_time": "2018-11-25T06:45:27.541768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Project Gutenberg Header\n",
    "ti_text = (req.text.split(\n",
    "    'START OF THIS PROJECT GUTENBERG EBOOK'))[-1]\n",
    "ti_text = ti_text.strip()\n",
    "ti_text = '\\n'.join(ti_text.split('\\r\\n')[6:])\n",
    "ti_text = ti_text.strip()\n",
    "ti_text[-250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:27.624996Z",
     "start_time": "2018-11-25T06:45:27.584918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Project Gutenberg Header\n",
    "ti_text = (req.text.split(\n",
    "    'START OF THIS PROJECT GUTENBERG EBOOK'))[-1]\n",
    "ti_text = ti_text.strip()\n",
    "ti_text = '\\n'.join(ti_text.split('\\r\\n')[6:])\n",
    "ti_text = ti_text.strip()\n",
    "# Remove Footer\n",
    "ti_text = ti_text.split('*** END OF THIS PROJECT GUTENBERG')[0]\n",
    "ti_text[-250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:27.669661Z",
     "start_time": "2018-11-25T06:45:27.628507Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Project Gutenberg Header\n",
    "ti_text = (req.text.split(\n",
    "     'START OF THIS PROJECT GUTENBERG EBOOK'))[-1]\n",
    "ti_text = ti_text.strip()\n",
    "ti_text = '\\n'.join(ti_text.split('\\r\\n')[6:])\n",
    "ti_text = ti_text.strip()\n",
    "# Remove Footer\n",
    "ti_text = ti_text.split('*** END OF THIS PROJECT GUTENBERG')[0]\n",
    "ti_text = '\\n'.join(ti_text.split('\\n')[:-3])\n",
    "ti_text[-250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:27.749328Z",
     "start_time": "2018-11-25T06:45:27.673291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "# Tokenizing the text\n",
    "ti_tokens = tokenizer.tokenize(ti_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:27.804756Z",
     "start_time": "2018-11-25T06:45:27.753537Z"
    }
   },
   "outputs": [],
   "source": [
    "# A new list to hold the lowercased words\n",
    "ti_words = [word.lower() for word in ti_tokens]\n",
    "\n",
    "# Printing out the first 8 words / tokens \n",
    "# ... YOUR CODE FOR TASK 5 ...\n",
    "ti_words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:27.837423Z",
     "start_time": "2018-11-25T06:45:27.808297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting the English stop words from nltk\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "# Printing out the first eight stop words\n",
    "# ... YOUR CODE FOR TASK 6 ...\n",
    "sw[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:28.070166Z",
     "start_time": "2018-11-25T06:45:27.840469Z"
    }
   },
   "outputs": [],
   "source": [
    "# A new list to hold Moby Dick with No Stop words\n",
    "ti_words_ns = [word for word in ti_words\n",
    "                  if word not in sw]\n",
    "\n",
    "# Printing the first 5 words_ns to check \n",
    "# that stop words are gone\n",
    "# ... YOUR CODE FOR TASK 7 ...\n",
    "ti_words_ns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:28.497186Z",
     "start_time": "2018-11-25T06:45:28.074554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the word frequency distribution\n",
    "ti_freqdist = nltk.FreqDist(ti_words_ns)\n",
    "\n",
    "# Plotting the word frequency distribution\n",
    "# ... YOUR CODE FOR TASK 8 ...\n",
    "ti_freqdist.plot(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:28.510573Z",
     "start_time": "2018-11-25T06:45:28.501792Z"
    }
   },
   "outputs": [],
   "source": [
    "for word, frequency in ti_freqdist.most_common(10):\n",
    "    print(word, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:28.540002Z",
     "start_time": "2018-11-25T06:45:28.514341Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_words(url):\n",
    "    req = requests.get(url)\n",
    "    # Remove Project Gutenberg Header\n",
    "    text = (req.text.split(\n",
    "        'START OF THIS PROJECT GUTENBERG EBOOK'))[-1]\n",
    "    text = text.strip()\n",
    "    text = '\\n'.join(text.split('\\r\\n')[6:])\n",
    "    text = text.strip()\n",
    "    # Remove Footer\n",
    "    text = text.split('*** END OF THIS PROJECT GUTENBERG')[0]\n",
    "    text = '\\n'.join(text.split('\\n')[:-3])\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "    words_ns = [word for word in words\n",
    "                  if word not in sw]\n",
    "    return text, words, words_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:29.415487Z",
     "start_time": "2018-11-25T06:45:28.544471Z"
    }
   },
   "outputs": [],
   "source": [
    "text, words, words_ns = text_to_words(ti_url)\n",
    "freqdist = nltk.FreqDist(words_ns)\n",
    "freqdist.plot(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:30.082536Z",
     "start_time": "2018-11-25T06:45:29.419245Z"
    }
   },
   "outputs": [],
   "source": [
    "jh_text, jh_words, jh_words_ns = text_to_words(jh_url)\n",
    "jh_freqdist = nltk.FreqDist(jh_words_ns)\n",
    "jh_freqdist.plot(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:30.092723Z",
     "start_time": "2018-11-25T06:45:30.085942Z"
    }
   },
   "outputs": [],
   "source": [
    "for word, frequency in jh_freqdist.most_common(10):\n",
    "    print(word, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:31.941827Z",
     "start_time": "2018-11-25T06:45:30.096561Z"
    }
   },
   "outputs": [],
   "source": [
    "md_text, md_words, md_words_ns = text_to_words(md_url)\n",
    "md_freqdist = nltk.FreqDist(md_words_ns)\n",
    "md_freqdist.plot(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:31.960802Z",
     "start_time": "2018-11-25T06:45:31.946831Z"
    }
   },
   "outputs": [],
   "source": [
    "for word, frequency in md_freqdist.most_common(10):\n",
    "    print(word, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:32.020900Z",
     "start_time": "2018-11-25T06:45:31.968263Z"
    }
   },
   "outputs": [],
   "source": [
    "number_mc=150\n",
    "word_df = pd.DataFrame({'md_words': [ item[0] for item in \n",
    "                        md_freqdist.most_common(number_mc)],\n",
    "                        'md_freqs':[ item[1] for item in\n",
    "                        md_freqdist.most_common(number_mc)],\n",
    "                        'ti_words': [ item[0] for item in\n",
    "                        ti_freqdist.most_common(number_mc)],\n",
    "                        'ti_freqs':[ item[1] for item in\n",
    "                        ti_freqdist.most_common(number_mc)],\n",
    "                        'jh_words': [ item[0] for item in\n",
    "                        jh_freqdist.most_common(number_mc)],\n",
    "                        'jh_freqs':[ item[1] for item in\n",
    "                        jh_freqdist.most_common(number_mc)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:32.073496Z",
     "start_time": "2018-11-25T06:45:32.025510Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:47:04.354305Z",
     "start_time": "2018-11-25T06:47:02.815954Z"
    }
   },
   "outputs": [],
   "source": [
    "mobyDict = dict(zip(word_df['md_words'],\n",
    "                    word_df['md_freqs']))\n",
    "\n",
    "wc = WordCloud(background_color=\"black\", \n",
    "               width=400,height=800,\n",
    "               max_words=150)\n",
    "wc.generate_from_frequencies(mobyDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:47:08.589319Z",
     "start_time": "2018-11-25T06:47:08.202430Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,figsize=(6,10))\n",
    "ax.imshow(wc)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:34.072306Z",
     "start_time": "2018-11-25T06:45:34.065207Z"
    }
   },
   "outputs": [],
   "source": [
    "word_clouds_dicts =[mobyDict]\n",
    "word_clouds_dicts.append(dict(zip(word_df['ti_words'],\n",
    "                                  word_df['ti_freqs'])))\n",
    "word_clouds_dicts.append(dict(zip(word_df['jh_words'],\n",
    "                                  word_df['jh_freqs'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T06:45:39.591085Z",
     "start_time": "2018-11-25T06:45:34.075768Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(12,20))\n",
    "for cloud_dict, ax in zip(word_clouds_dicts,axs):\n",
    "    wc = WordCloud(background_color=\"black\",width=400,height=800, max_words=150)\n",
    "    wc.generate_from_frequencies(cloud_dict)\n",
    "    ax.imshow(wc)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
